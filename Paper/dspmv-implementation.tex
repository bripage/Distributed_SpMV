\section{Implementation}\label{sec:dspmv-implementation}

%- unlike Bylina et al's multinodal algorithm, which used BLACS to perform the data distribution amongst the cluster, as MKL to perfrom the multithreaded SpMV on each cluster node, a hybrid distrbuted SpMV was designed as a %single stand alone entity. 
%	- control over all MPI and OpenMP operations, including thread and process affinity, resource over scheduling, etc.
%	- does not require software licensing
%	- can be tailored to cluster size and node hardware architecture
%	
%- the application was written in c++, compiled with openmpi with passthrough to the g++ compiler
%	- compiled with openmp 
%	- optimization level 3
%	
%- Careful attention was paid to the MPI communication structure such that it would emulate the behavior produced by the BLACS routines used by Bylina et al for work distribution amongst cluster nodes. 
%	- this is not optimized, however does allow for greater control in evaluating potential scalability bottlenecks stemming from communication overhead
%	
%- Briefly discuss program flow/operation
%	- similar to Bylinia et al the cluster is split into a grid consisting of $n$ x $n$ nodes in thereby requiring $n^2$ number of nodes in the cluster to function properly.
%	- distribution determined on master, then sent to cluster column masters, and finally column masters send to remaining column members. 
%	- once data has been recieved by individual nodes, they perform a multi-threaded SpMV algorithm which utilizes OpenMP for parallelization
%		- each thread works on a block of rows from the submatrix portion assigned to the MPI process
%		- all threads have same number of rows in their blocks, though not necessarily the same number of non-zeros to work on due to matrix distrubtion
%	- vectors provide potential increase in locality of reference due to being allcated contiguous portions of memory \TODO{you will likely need to cite this}
%	- once all nodes in a cluster row have completed computation of their assigned work, an MPI\_Reduce is called and performs a summation with the results being saved into a vector at the cluster row master for each %cluster row. 
%	- the 0th column of cluster nodes contains all cluster row masters including the Global Master. It is on this cluster column that MPI\_Gather is called in order to collect the distributed results back at the global %master process. 
%
\subsection{Work Distribution and MPI}

The application that was written to emulate the behavior of that used in the study performed by Bylina et al was written using the C++ programming language.
We chose to forgo the use of proprietary libraries such as BLACS and MKL deciding instead to write explicit MPI and OpenMP directives to control distributed and shared memory behavior across the cluster environment. 
Special care was paid to insure that the communication pattern matched that of the 2d cluster methods in the BLACS library which were used in the prior work.
It was felt that by not including these packages greater control over communication and memory access parameters could be achieved, even though the result may not be as highly optimized for particular applications, architectures, or compilers. 


Benchmark matrices in the Matrix Market Format were chosen from the University of Florida Sparse Matrix Collection. The characteristics of the matrices chosen is discussed in greater detail in section 4a "Benchmarks". 
Matrices are read from file by the master MPI process and converted to Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) format, depending on the individual matrix being input.
Both CSR and CSC formats provide a reduction on memory requirements thereby increasing performance while reducing data transfer in multinode environments since only data about non-zero elements in the input matrix are kept \TODO{this needs a cite}. 
While the matrix is being read in, the distribution of work amongst the MPI processes is being determined. 
The distribution pattern used by Bylina et al and which we have emulated, we call the \emph{sub matrix method}, splits the input matrix $A$ into $p^2$ sub matrices in which each piece has nearly identical dimensions based on the number of processes $p$ and the row or column count of $A$.
Therein the size of each sub matrix from $A_p$ will be $A_{rows}/\sqrt{p}$  x $A_{cols}/\sqrt{p}$. 
We chose to require that $p$ be a non-negative square value.

The csrSpMV class was created to contain submatrix information in CSR/CSC format so that work allocation accross MPI processes could be performed prior to MPI communication amongst those processes taking place, and stores information about each non-zero in three vectors. 
As each new non-zero is read the sub matrix it is to be assigned to is easily determined from its row and column, and is subsequently added to the csrSpMV object representing that submtrix. 


Given that there are the same number of processes as submatrices, $p$, we can view the processes as being laid out in a matrix $P$ in which process $P_{ij}$ will receive data corresponding to submatrix $A_{ij}$.
The first row of the process matrix containing processes with global MPI ranks $0$ to $p-1$ are termed \emph{column masters}, while the first column of processes with global ranks such that $rank \% \sqrt{p} $ are \emph{row masters}.
The MPI master process sends every column master the data contained within the csrSpMV object containg all data to be destributed amongts its column.
Column masters will then send information about non-zeros to each process in its column.
Each process receives its work allotment, if any, and proceeds to computation.

It is important to note that given the \emph{sub matrix} distribution method considerable work imbalances including process with no work can occur between processes. As discussed in Bylina et al such imbalances can lead to entire computing nodes sitting idle as they have no data to process, thereby potentially decreasing overall performance. 

\subsection{OpenMP SpMV}

\begin{algorithm}
\caption{Hybrid SpMV}\label{spmv}
\begin{algorithmic}[1]
	\Procedure{OpenMP SpMV}{}
	\State \textbf{Input}: csrSpMV $\textit{nodeCSR}$, int $\textit{rowsPerThread}$
	\medskip
	\State $threadId \gets omp\_get\_thread\_num()$
	\State $rStart \gets threadId*rowsPerThread$
	\medskip
	
	\If {$threadId$ $==$ $threadCount-1$}
		\State $rEnd \gets nodeCSR.Rows.size()$
	\Else 
		\State $rEnd \gets (threadId+1) * rowsPerThread$
	\EndIf
	\medskip
	
	\For{$i \gets rStart - rEnd$}
		\State $dStart \gets nodeCSR.Rows[i]$
		
		\If {$i$ $==$ $rEnd-1$}
			\State $dEnd \gets nodeCSR.Data.size()$
		\Else 
			\State $dEnd \gets nodeCSR.Rows[i+1]$
		\EndIf
			
		\For{$j \gets dStart - dEnd$}
			\State $\textit{result[i] += nodeCSR.Data[j] * nodeCSR.denseVec[i]}$
		\EndFor
	\EndFor
	
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{spmv} shows the procedure used to perform the multithreaded SpMV computation within the OpenMP pragma section of the program. Each process performs this algorithim using the number of OpenMP threads set by the administrator at runtime via command line parameters. 
The OpenMP pragma establishes several shared and private variables that can be accessed by an individual thread. 

\bigskip
\#pragma omp parallel num\_threads(control.ompThreads) shared(nodeCSR, result) private(ompThreadId, start, end, i, j, rowsPerThread)
\bigskip

As seen in the pragma above, the number of threads created is a value set at runtime and stored in the ompThreads variable with the control structure which contains other paramaters needed for distribution and control of the application.
The csrSpMV object containing that particular node's data, called $nodeCSR$, is shared amongst all threads as is the $result$ vector.
We can share these items since the data contained with $nodeCSR$ will only read from, and each individual thread will only acess the elements of $result$ corresponding to the rows which it has been assigned to work on.
All other variables explicitly listed as private are necessary to insure each thread has a copy within its memory space without the possibility of being overwritten.
Each process carries out the SpMV procedure once it is acquired all data from its column master and is ready to proceed with computation. 


\subsection{Reduction and Validation}

Upon completion of computation all nodes within a row perform an MPI recution where each processes results are summed and stored within the row master's \emph{result} vector.
This is possible as each processes within the same row of the process matrix are working on the same rows from $A$, but only on those non-zero elements contained within their assigned submatrix from $A$. 
After having performed the reduction a gather is performed on the process column contaning the global master process (also a row master), and all other row masters.
At this point the global master process now has all results and can proceed with secondary computation if necessary. 

During development it was necessary to insure that the hybrid portion of the program was computing the correct SpMV result for a given matrix $A$ and a dense vector.
In order to verify accuracy of the hybrid version, a sequential version of the SpMV algorithm was performed on the master process only, prior to the hybrid portion of the program being performed.
The results from each method were then compared and any differences indicated an error in computation.
This was performed with a series of matrices, increasing in size, until no descrepencies were found amongst the test matrices. 
With the validity of the hybrid algorithms communication and computation tested and verified, the sequential master only computation was removed so that benchmarking tests could be performed. 

