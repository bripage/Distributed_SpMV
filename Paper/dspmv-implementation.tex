\section{Experimental Implementation}\label{sec:dspmv-implementation}

%- unlike Bylina et al's multinodal algorithm, which used BLACS to perform the data distribution amongst the cluster, as MKL to perfrom the multithreaded SpMV on each cluster node, a hybrid distrbuted SpMV was designed as a %single stand alone entity. 
%	- control over all MPI and OpenMP operations, including thread and process affinity, resource over scheduling, etc.
%	- does not require software licensing
%	- can be tailored to cluster size and node hardware architecture
%	
%- the application was written in c++, compiled with openmpi with passthrough to the g++ compiler
%	- compiled with openmp 
%	- optimization level 3
%	
%- Careful attention was paid to the MPI communication structure such that it would emulate the behavior produced by the BLACS routines used by Bylina et al for work distribution amongst cluster nodes. 
%	- this is not optimized, however does allow for greater control in evaluating potential scalability bottlenecks stemming from communication overhead
%	
%- Briefly discuss program flow/operation
%	- similar to Bylinia et al the cluster is split into a grid consisting of $n$ x $n$ nodes in thereby requiring $n^2$ number of nodes in the cluster to function properly.
%	- distribution determined on master, then sent to cluster column masters, and finally column masters send to remaining column members. 
%	- once data has been recieved by individual nodes, they perform a multi-threaded SpMV algorithm which utilizes OpenMP for parallelization
%		- each thread works on a block of rows from the submatrix portion assigned to the MPI process
%		- all threads have same number of rows in their blocks, though not necessarily the same number of non-zeros to work on due to matrix distrubtion
%	- vectors provide potential increase in locality of reference due to being allcated contiguous portions of memory \TODO{you will likely need to cite this}
%	- once all nodes in a cluster row have completed computation of their assigned work, an MPI\_Reduce is called and performs a summation with the results being saved into a vector at the cluster row master for each %cluster row. 
%	- the 0th column of cluster nodes contains all cluster row masters including the Global Master. It is on this cluster column that MPI\_Gather is called in order to collect the distributed results back at the global %master process. 
%
\subsection{Work Distribution and MPI}

Communication and computational behavior of a multi-nodal solver developed for a previous study \cite{techbib:6933066} was used as the basis for our implementation
Unlike \cite{techbib:6933066}, we chose to forgo the use of proprietary libraries such as BLACS and MKL deciding instead to write explicit MPI and OpenMP directives to control distributed and shared memory behavior across the cluster environment. 
It was felt that exclusion of these libraries provided, greater control over program operation throughout our evaluation.
Special care was paid to insure that the communication pattern matched that of the 2D cluster methods in the BLACS library which were used in the prior work.
 

Matrices, stored in Matrix Market Format (*.mtx), are read from file by the master MPI process and converted to Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) format, depending on the input matrix.
CSR and CSC formats provide a reduced memory requirements, thereby increasing performance while reducing data transfer in multi-node environments since only data about non-zero elements in the input matrix are kept. 
While the matrix is being read in, the distribution of work amongst the process matrix is being determined. 
The \emph{sub matrix method} used in \cite{techbib:6933066} breaks the input matrix $A$ into $p^2$ sub matrices with nearly identical dimensions based on the number of rows/cols in teh process matrix and the row/col count of $A$.
We chose to require that $p$ be a non-negative square value, therein the size of each sub-matrix from $A_p$ will be $A_{rows}/p$  x $A_{cols}/p$. 


The csrSpMV class was created to contain sub-matrix information in CSR/CSC format so that work allocation across MPI processes could be performed prior to MPI communication amongst the process matrix taking place. 
As each non-zero is read, its row and column are used to determine which $A_p$ it will be assigned to, and is subsequently added to the csrSpMV object representing that sub-matrix 


The first row of the process matrix containing processes with global MPI ranks $0$ to $(p-1)$ are termed \emph{column masters}, while the first column of processes with global rank such that $rank$ \% $\sqrt{p}$ are \emph{row masters}.
The MPI master process sends every column master all data to be distributed amongst its column.
Column masters will then transmit each process in its column their individual work allotment, wherein after receipt each process proceeds to computation.

It is important point out considerable work imbalances including process with no work can occur between processes in the \emph{sub matrix} distribution method. As discussed in \cite{techbib:6933066} such imbalances can lead to entire computing nodes sitting idle as they have no data to process, thereby potentially decreasing overall performance. 

\subsection{OpenMP SpMV}

\begin{algorithm}
\caption{Hybrid SpMV}\label{alg:spmv}
\begin{algorithmic}[1]
	\Procedure{OpenMP SpMV}{}
	\State \textbf{Input}: csrSpMV $\textit{nodeCSR}$, int $\textit{rowsPerThread}$
	\medskip
	\State $threadId \gets omp\_get\_thread\_num()$
	\State $rStart \gets threadId*rowsPerThread$
	\medskip
	
	\If {$threadId$ $==$ $threadCount-1$}
		\State $rEnd \gets nodeCSR.Rows.size()$
	\Else 
		\State $rEnd \gets (threadId+1) * rowsPerThread$
	\EndIf
	\medskip
	
	\For{$i \gets rStart - rEnd$}
		\State $dStart \gets nodeCSR.Rows[i]$
		
		\If {$i$ $==$ $rEnd-1$}
			\State $dEnd \gets nodeCSR.Data.size()$
		\Else 
			\State $dEnd \gets nodeCSR.Rows[i+1]$
		\EndIf
			
		\For{$j \gets dStart - dEnd$}
			\State $\textit{result[i] += nodeCSR.Data[j]$\
				$* nodeCSR.denseVec[i]}$
		\EndFor
	\EndFor
	
	\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:spmv} shows the procedure used to perform the multi-threaded SpMV computation within the OpenMP pragma. Each process performs this algorithm with the number of OpenMP threads established at runtime via command line parameters, along with several shared and private variables that can be accessed by an individual thread. 

%\bigskip
%\#pragma omp parallel num\_threads(control.ompThreads) shared(nodeCSR, result) private(ompThreadId, start, end, i, j, rowsPerThread)
%\bigskip

%As seen in the pragma above, the number of threads created is a value set at runtime and stored in the ompThreads variable with the control structure which contains other paramaters needed for distribution and control of the application.
The csrSpMV object containing that particular processes data, called $nodeCSR$, is shared amongst all threads as is the $result$ vector.
It is appropriate to share data in such a manner since data contained within $nodeCSR$ is only read from, while individual threads access only those elements of $result$ corresponding to the rows to which they have been assigned.
All other variables explicitly listed as private are necessary to ensure each thread has a copy within its memory space without the possibility of being overwritten.
Each process carries out the SpMV procedure once it has acquired all data from its column master and is ready to proceed with computation. 

\subsection{Reduction and Validation}

Upon completion of computation all processes within a row perform an MPI reduction in which results are summed and stored within the row master's \emph{result} vector.
After having performed the reduction a gather is performed on the process column containing the global master process (a row master), as well as all other row masters.
Finally the global master process has all results and can proceed with secondary computation if necessary. 

During development it was necessary to ensure that the hybrid portion of the program was computing the correct SpMV result for a given matrix $A$ and a dense vector.
In order to verify accuracy of the hybrid version, a sequential version of the SpMV algorithm was performed on the master process only, prior to the hybrid portion of the program being performed.
The results from each method were then compared and any differences indicated an error in computation.
This was performed with a series of matrices, increasing in size, until no discrepancies were found amongst the test matrices. 
With the validity of the hybrid algorithm's communication and computation tested/verified, the sequential computation was removed so that evaluation could be proceed. 

