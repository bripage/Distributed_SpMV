The product of a sparse matrix and a dense vector (\textbf{SpMV}) is a key part of many codes, and a variety of studies have shown that the major driver to performance is memory bandwidth, and not peak floating point potential. One recent study in particular looked at strong scaling of a variety of matrices of varying sparsity, with performance that often declines with increasing parallelism. This paper reports both a more detailed analytic model and a carefully instrumented hybrid implementation using both OpenMP and MPI that explore these odd behaviors and identify the causes. Together, this should set the stage for better scaling algorithms on more advanced platforms.