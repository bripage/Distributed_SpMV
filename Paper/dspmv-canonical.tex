\section{Canonical Graph Processing}\label{sec:sga-canonical}

\begin{figure*}\begin{centering}
\includegraphics{../figures/sga-canonical-flow-revised.eps}
\caption{A Canonical Graph Processing Flow.}
\label{fig:sga-canonical-flow}
\end{centering}\end{figure*}

While useful in an academic setting, the kernels and benchmarks discussed in the prior section are missing some major attributes if they are to be used to help understand the characteristics of systems that run real, more complex, graph applications. This section discusses a canonical processing flow that is perhaps closer to real practice, and demonstrates the potential interaction of multiple kernels with more realistic data structures. The goal for the discussion is that there is room for significant work in defining benchmarks that are more complex than any of the existing benchmark suites.

Fig. \ref{fig:sga-canonical-flow} diagrams a processing flow that includes both batch (middle and right side) and streaming graph processing (left side) in a more realistic combination.

Unlike many of the kernels discussed in the prior section, real applications start with large graphs built from not one but many classes of vertices and edges, often of multi-terabyte size, with thousands of properties per vertex, and time-stamps on edges. These graphs are persistent; their existence is independent of any single analytic or set of analytics. Today, these graphs are initially created via some large batch processing \textbf{dedup} processes \cite{big-data:dedup:5887335,big-data:dedup:4016511} that ``clean up'' multiple data sets by checking spelling, removing duplicates (\emph{post-process deduping}), identifying faulty or missing values, and combining to create properties to be associated with vertices and edges. In a streaming form called \emph{in-line deduping}, once established, updates will be from streams of incoming data. In Fig. \ref{fig:sga-canonical-flow} bulk undeduped data enters from the bottom, with the deduping process handled by a batch analytic before being used to build the initial graph.

Once a graph is established, real application analytics typically are not run over the entire graph. Instead, an analytic may start with identification of some  \textbf{selection criteria} (on right) that are used to identify some initial \textbf{seed} entries. This may be as simple as specifying some particular vertex, or more involved such as scanning for the ``top k'' vertices with the highest values of some properties.

Once identified, these seeds may then be used to perform some sort of \textbf{subgraph extraction}, whereby a subset of the large graph is identified. A simple example of such a process may be a breadth-first search from individual seed vertices out to some depth, or perhaps out some distance from some path between two or more seeds. Once identified, it may be appropriate to physically copy such a subgraph out of the memory holding the large persistent graph into a smaller, but faster access rate, memory from which more complex analytics can be run. This copy process may also include some sort of projection to copy only a small subset of the properties.

Potentially long-running batch analytics may then be run on these subgraphs. Typical outputs may be metrics of the overall graph and/or even smaller subgraphs or vertex/edge sets (``neighborhoods''). Of growing importance, however, is the use of the analytic to compute/update properties of vertices or edges that are to be sent back to update the original persistent graph. This is in fact how many real-world applications end up with thousands of vertex properties, as analysts often find that some new property of vertices is useful, and it is easier to define a ``one-time'' analytic that computes this property for all vertices at once, and then use the property values in later repeated calls to application-specific analytics.

Stream processing (left-hand side) takes a different path. A real-time stream of data arrives incrementally. Processing may take a variety of forms. First, the inputs may specify specific vertices and some update to one or more of the vertex's properties. This is the case for the Firehose benchmark \cite{sga-firehose}. Alternatively, inputs may specify new edges (less frequently new vertices). Processing either one may involve checking if it is already in the graph and then either adding the edge or updating some properties associated with an existing edge. Less often are deletions. A good example of this is the in-line deduping process discussed earlier.

In both cases, the initial operations against the graph are relatively simple and rather local. However, it is not uncommon for the stream processing to look for changes in local or global graph parameters, and only if those parameters exceed some threshold, to use the modified vertices/edges as seeds into a subgraph extraction process similar to that described for the batch process. The extracted subgraph may then be the target of a specific batch analytic to more fully analyze the effects of the change. As before, the execution of this analytic may result in either alerts back to some external system and/or updates to properties in the larger graph.

An example of a  matching real-world application can be found in \cite{kogge-ref:NORA}. A 2012 batch-only implementation of an insurance application problem periodically combined and cleaned 40+ TB of public data into a persistent database of 4-7+ TB (bigger now). Once a week this data set is ``boiled'' (over the weekend) to pre-compute answers to a set of queries, in two forms and for each of all multi-million people in the set. The first form is a simple indexed database where simple SQL-like selects can quickly retrieve data relevant to a particular insurance applicant, such as credit score. The more valuable answers are, however, the result of searches for ``relationships'' between people, such as ``who has shared an address with what other individuals 2 or more times, especially if they have shared a common last name.'' These latter computations represent the bulk of the weekly computations, and are close to the Jaccard coefficient kernel mentioned earlier, and have been called \textbf{Non-Obvious Relationship Analysis} (\textbf{NORA}).

Such a resulting data set is then used, for example, to provide real-time data back to an insurance company when a potential client has entered data for a quote. This data includes both the specific client data and the results of NORA relationships involving the client, and is used as input to the company's quoting process.

A streaming real-time version of such applications would have both types of streaming as discussed above. One stream would be updates to the persistent graph, and determine for each updated vertex if the update is likely to change any of the key relationships. Simply adding more validity to a pre-identified relationship needs no more processing, but when there is the potential for crossing some threshold, a more complete computation of the particular metric may be warranted. Such a process makes the data set less stale.

The second type of streaming would take a sequence of applicants and compute in real-time whatever relationships are relevant for the type of application they are applying for. This has the significant advantage of removing much of the need for the pre-computation that takes so much time. It also increases the fidelity of the responses, as the results include updates since the last pre-computation.
